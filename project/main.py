import os
from dotenv import load_dotenv
# ------------------------- ÌôòÍ≤Ω ÏÑ§Ï†ï -------------------------
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), ".env"))
print("env ÌååÏùº ÏûàÏùå" if os.path.exists(os.path.join(os.path.dirname(__file__), ".env")) else "env ÌååÏùº ÏóÜÏùå",end=" ,")
print(f"ÌôòÍ≤Ω Î≥ÄÏàò openai_api_key Î°úÎî© ÏôÑÎ£å" if os.getenv("OPENAI_API_KEY") else "ÌôòÍ≤Ω Î≥ÄÏàò openai_api_key ÏóÜÏùå", end=" ,")

from fastapi import FastAPI, UploadFile, File, Request, Form
from fastapi.responses import FileResponse, HTMLResponse, JSONResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import shutil, torch, random

from rag_engine.processor.chunker import chunk_file
from rag_engine.processor.vector_saver import save_faiss_index, get_embeddings, save_meta_config
from config.config_manager import save_config, load_config
from rag_engine.processor.db_loader import load_log_file_to_sqlite

app = FastAPI()

# ------------------------- ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú -------------------------
BASE_DIR = "frontend"
PREPROCESSOR_DIR = os.path.join(BASE_DIR, "preprocessor")
CHATBOT_DIR = os.path.join(BASE_DIR, "chatbotsetting")
CHATTING_DIR = os.path.join(BASE_DIR, "chatting")

DATA_BASE = "data"
RAW_DIR = os.path.join(DATA_BASE, "raw_file")
CHUNK_BASE = {
    "manual": os.path.join(DATA_BASE, "manual", "processed_chunks"),
    "sql": os.path.join(DATA_BASE, "sqlguide", "processed_chunks")
}
VECTOR_BASE = {
    "manual": os.path.join(DATA_BASE, "manual", "vector_store"),
    "sql": os.path.join(DATA_BASE, "sqlguide", "vector_store")
}

os.makedirs(RAW_DIR, exist_ok=True)
for path in list(CHUNK_BASE.values()) + list(VECTOR_BASE.values()):
    os.makedirs(path, exist_ok=True)

# ------------------------- Ï†ïÏ†Å ÌååÏùº ÎßàÏö¥Ìä∏ -------------------------
app.mount("/frontend", StaticFiles(directory=BASE_DIR), name="frontend")
app.mount("/config", StaticFiles(directory="config"), name="config")
# ------------------------- ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï -------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"[INFO] Using device: {device}")

# ------------------------- ÏÑ§Ï†ï API -------------------------

@app.get("/chat_config")
def get_chat_config():
    return load_config()

@app.post("/chat_config")
async def set_chat_config(request: Request):
    new_config = await request.json()
    current_config = load_config()

    # üîÅ Í∏∞Ï°¥ ÏÑ§Ï†ïÍ≥º Î≥ëÌï©
    merged_config = {**current_config, **new_config}
    
    save_config(merged_config)
    return {"message": "‚úÖ ÏÑ§Ï†ïÏù¥ Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§."}


# ------------------------- Î©îÏù∏ ÌôîÎ©¥ -------------------------

@app.get("/")
def serve_main():
    return RedirectResponse(url="/frontend/index.html")


# ------------------------- Îç∞Ïù¥ÌÑ∞ ÏóÖÎ°úÎìú Î∞è Î≤°ÌÑ∞ Ï†ÄÏû• -------------------------

@app.post("/upload_log")
def upload_log(
    file: UploadFile = File(...),
    chunk_size: int = Form(200),
    chunk_overlap: int = Form(0),
    embedding_model: str = Form("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"),
    vector_type: str = Form("FAISS"),
    process_type: str = Form("manual")
):
    filename = file.filename
    file_path = os.path.join(RAW_DIR, filename)
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    # Ï≤≠ÌÅ¨ Î∞è Î≤°ÌÑ∞ Ï†ÄÏû• Í≤ΩÎ°ú
    chunk_dir = CHUNK_BASE.get(process_type)
    vector_dir = VECTOR_BASE.get(process_type)

    # ÌÖçÏä§Ìä∏ chunk Ï≤òÎ¶¨
    chunks = chunk_file(file_path, chunk_dir, chunk_size, chunk_overlap)

    # FAISS Ïù∏Îç±Ïä§ Ï†ÄÏû•
    index_path = os.path.join(vector_dir, "index.faiss")
    save_faiss_index(chunks, index_path, model_name=embedding_model)

    # Î©îÌÉÄ Ï†ïÎ≥¥ Ï†ÄÏû•
    vector_dim = len(get_embeddings(["ÏûÑÏãú"], embedding_model)[0])
    save_meta_config(
        save_dir=vector_dir,
        embedding_model=embedding_model,
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        vector_type=vector_type,
        process_type=process_type,
        vector_dim=vector_dim,
        filename=filename
    )

    # ÏÉòÌîå ÌÖçÏä§Ìä∏ ÎØ∏Î¶¨Î≥¥Í∏∞
    sample_text = ""
    try:
        chunk_files = [f for f in os.listdir(chunk_dir) if f.endswith(".txt")]
        if chunk_files:
            random_file = random.choice(chunk_files)
            with open(os.path.join(chunk_dir, random_file), "r", encoding="utf-8") as f:
                sample_text = f.read()
    except Exception as e:
        sample_text = f"ÎØ∏Î¶¨Î≥¥Í∏∞ Ïò§Î•ò: {str(e)}"

    return {
        "status": "uploaded and processed",
        "filename": filename,
        "device": str(device),
        "sample": sample_text.strip()
    }

# ------------------------- ÏßàÎ¨∏ ÏùëÎãµ API -------------------------

class QueryRequest(BaseModel):
    question: str

from fastapi.responses import JSONResponse
from rag_engine.chain.full_chain import build_full_chain
from config.config_manager import load_config
from langchain_openai import ChatOpenAI
import traceback, re
from rag_engine.chain.full_chain_loader import get_cached_full_chain

@app.post("/ask")
def ask_question(query: QueryRequest):
    try:
        full_chain = get_cached_full_chain()
        if not full_chain:
            return JSONResponse(content={"error": "Ï≤¥Ïù∏Ïù¥ Ï¥àÍ∏∞ÌôîÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§."}, status_code=500)

        result = full_chain.invoke(
            {"question": query.question},
            config={"configurable": {"session_id": "default"}}
        )
        response = result.get("response", "")
        clean_response = re.sub(r"[*_#`]", "", response)
        return JSONResponse(content={"answer": clean_response})

    except Exception as e:
        traceback.print_exc()
        return {"error": str(e), "trace": traceback.format_exc()}



# ------------------------- Î≤°ÌÑ∞ ÏÉÅÌÉú ÌôïÏù∏ -------------------------

@app.get("/api/vector_status")
def get_vector_status():
    import glob

    result = {}

    # ‚úÖ LOG: .db ÌååÏùº Í∏∞Ï§Ä
    log_db_files = glob.glob("data/factory/*.db")
    result["log"] = {
        "loaded": len(log_db_files) > 0,
        "doc_count": len(log_db_files)
    }

    # ‚úÖ SQL, MANUALÏùÄ Í∏∞Ï°¥ Î∞©Ïãù Ïú†ÏßÄ
    for key in ["manual", "sql"]:
        try:
            index_path = os.path.join(VECTOR_BASE[key], "index.faiss")
            loaded = os.path.exists(index_path)

            chunk_dir = CHUNK_BASE.get(key, "")
            chunk_files = os.listdir(chunk_dir) if os.path.isdir(chunk_dir) else []
            doc_count = len([f for f in chunk_files if f.endswith(".txt")])

            result[key] = {
                "loaded": loaded,
                "doc_count": doc_count
            }
        except Exception as e:
            result[key] = {
                "loaded": False,
                "doc_count": 0,
                "error": str(e)
            }

    return JSONResponse(content=result)



# ------------------------- db ÏÉùÏÑ± -------------------------
@app.post("/upload_log_to_db")
def upload_log_to_sqlite(file: UploadFile = File(...)):
    return load_log_file_to_sqlite(file)
