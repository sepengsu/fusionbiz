from sentence_transformers import SentenceTransformer, util
import numpy as np

# ðŸ”¸ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
SQL_KEYWORDS = ["ì¿¼ë¦¬", "sql", "select", "insert", "delete", "í…Œì´ë¸”", "where", "join"]
MANUAL_KEYWORDS = ["ì—ëŸ¬", "ì˜¤ë¥˜", "ì •ì§€", "ê³ ìž¥", "ìž‘ë™", "ë©ˆì¶¤", "ëŒ€ì‘", "ë§¤ë‰´ì–¼", "ìš´ì „"]

def classify_by_keyword(question: str) -> str:
    q = question.lower()
    for kw in SQL_KEYWORDS:
        if kw in q:
            return "sql"
    for kw in MANUAL_KEYWORDS:
        if kw in q:
            return "manual"
    return "manual"  # default fallback

# ðŸ”¸ ìž„ë² ë”© ê¸°ë°˜ ëŒ€í‘œ ë¬¸ìž¥
prototype_sql = [
    "ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ëŠ” SQL ì¿¼ë¦¬",
    "ìš´ì „ ë¡œê·¸ë¥¼ í…Œì´ë¸”ë¡œ ë³´ì—¬ì¤˜",
    "ì •ì§€ íšŸìˆ˜ë¥¼ ì•Œë ¤ì¤˜",
    "ì—ëŸ¬ ë¡œê·¸ ê°œìˆ˜ë¥¼ í™•ì¸í•˜ê³  ì‹¶ì–´",
    "1ì›” 29ì¼ ëª‡ ë²ˆ ì •ì§€ëëŠ”ì§€ ì•Œë ¤ì¤˜"
]

prototype_manual = [
    "ì™œ ê³ ìž¥ë‚¬ëŠ”ì§€ ì•Œì•„ë´ì¤˜",
    "ì •ì§€ ì´ìœ ê°€ ë­ì•¼?",
    "ìž‘ë™ ì¤‘ ë©ˆì¶˜ ì´ìœ ëŠ”?",
    "ê¸°ê³„ê°€ ì•ˆ ë¼. ì ê²€ì€ ì–´ë–»ê²Œ í•´?",
    "ì •ë¹„ ë°©ë²•ì´ ì–´ë–»ê²Œ ë¼?"
]

_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

def classify_by_embedding_pairwise(question: str) -> str:
    q_vec = _model.encode(question)
    sql_vecs = _model.encode(prototype_sql)
    manual_vecs = _model.encode(prototype_manual)

    sql_score = util.cos_sim(q_vec, sql_vecs).mean().item()
    manual_score = util.cos_sim(q_vec, manual_vecs).mean().item()

    return "sql" if sql_score > manual_score else "manual"

# ðŸ”¸ ìµœì¢… í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ê¸°
def classify_question_type(question: str) -> str:
    kw_label = classify_by_keyword(question)
    if kw_label != "manual":  # í‚¤ì›Œë“œë¡œ sqlë¡œ í™•ì‹ ë˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        return kw_label
    return classify_by_embedding_pairwise(question)

def classify_question_type(question: str, use_llm: bool = False) -> str:
    """
    ê¸°ë³¸ì ìœ¼ë¡œ í‚¤ì›Œë“œ + ìž„ë² ë”© ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰.
    use_llm=Trueì¼ ê²½ìš°, LLM (ì˜ˆ: GPT) ê¸°ë°˜ ë¶„ë¥˜ê¸°ë¡œ ëŒ€ì²´ ê°€ëŠ¥ (ë‚˜ì¤‘ì— í™•ìž¥ìš©)
    """
    if not use_llm:
        kw_label = classify_by_keyword(question)
        if kw_label != "manual":  # í‚¤ì›Œë“œë¡œ sqlë¡œ í™•ì‹ ë˜ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            return kw_label
        return classify_by_embedding_pairwise(question)

    # âœ³ï¸ í–¥í›„ LLM ì‚¬ìš© ì‹œ (ì˜ˆ: langchain ê¸°ë°˜)
    return classify_by_llm(question)

from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
import os

# ðŸ”¹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = PromptTemplate.from_template("""
ë‹¤ìŒ ì§ˆë¬¸ì„ "sql", "manual", "summary" ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•´ì¤˜.

ì§ˆë¬¸: "{question}"
ë‹µë³€ (sql/manual/summary ì¤‘ í•˜ë‚˜):
""")

# ðŸ”¹ LLM ëª¨ë¸ ì •ì˜
llm = ChatOpenAI(
    temperature=0,
    model="gpt-3.5-turbo",
    api_key=os.getenv("OPENAI_API_KEY")
)

# ðŸ”¹ ì²´ì¸ êµ¬ì„±: prompt â†’ LLM
chain = prompt | llm | RunnableLambda(lambda x: x.content.strip().lower())

# ðŸ”¹ ë¶„ë¥˜ í•¨ìˆ˜ ì •ì˜
def classify_by_llm(question: str) -> str:
    try:
        result = chain.invoke({"question": question})
        if result in ["sql", "manual", "summary"]:
            return result
    except Exception as e:
        print(f"[ERROR] LLM ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
    return "manual"


